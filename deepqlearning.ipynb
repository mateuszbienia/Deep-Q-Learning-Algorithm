{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from envs.connect4.Connect4Env import Connect4Env\n",
        "\n",
        "from ai.dqn.DeepNetworkModel import DeepNetworkModel\n",
        "from ai.dqn.DQN import DQN\n",
        "\n",
        "from agents.AgentRandom import AgentRandom\n",
        "from agents.AgentDQN import AgentDQN\n",
        "\n",
        "from tools import *\n",
        "\n",
        "import gym\n",
        "from gym.envs.registration import register\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZIYmzr4mnjg"
      },
      "outputs": [],
      "source": [
        "register(\n",
        "    id='Connect4Env-v0',\n",
        "    entry_point='envs.connect4.Connect4Env:Connect4Env',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q93ihIImmnjq"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Connect4Env-v0')\n",
        "\n",
        "episodes = 10_000\n",
        "learning_rate = 0.0001# best so far 0.0001\n",
        "gamma = 0.99\n",
        "\n",
        "input_layer = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', input_shape=(1, 3, 6, 7), data_format=\"channels_first\", kernel_regularizer='l1_l2', padding=\"same\")\n",
        "hidden_layers = []\n",
        "hidden_layers.append(tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='RandomNormal', kernel_regularizer='l1_l2', padding=\"same\"))\n",
        "hidden_layers.append(tf.keras.layers.Flatten())\n",
        "hidden_layers.append(tf.keras.layers.Dense(256, activation='relu', kernel_initializer='RandomNormal'))\n",
        "hidden_layers.append(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='RandomNormal'))\n",
        "\n",
        "batch_size = 256\n",
        "epsilon = 0.9999\n",
        "min_epsilon = 0.05\n",
        "epsilon_multiplier = 0.99976 # 10k\n",
        "max_memory = 10000\n",
        "min_memory = 260\n",
        "copy_step = 20\n",
        "copy_iter = 0\n",
        "\n",
        "total_turns = np.zeros(episodes)\n",
        "total_rewards = np.zeros(episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_state(self, state):\n",
        "    if state == None:\n",
        "        return None\n",
        "    if len(state) > 3:\n",
        "        states = []\n",
        "        for s in state:\n",
        "            new_state = s\n",
        "            player1 = np.where(new_state == 1, 1, 0).reshape((6,7))\n",
        "            player2 = np.where(new_state == 2, 1, 0).reshape((6,7))\n",
        "            empty = np.where(new_state == 0, 1, 0).reshape((6,7))\n",
        "            states.append(np.array([player1, player2, empty], dtype=np.uint8).reshape((1, 3,6,7)))\n",
        "        return np.asarray(states)\n",
        "    new_state = state\n",
        "    player1 = np.where(new_state == 1, 1, 0).reshape((6,7))\n",
        "    player2 = np.where(new_state == 2, 1, 0).reshape((6,7))\n",
        "    empty = np.where(new_state == 0, 1, 0).reshape((6,7))\n",
        "    return np.array([player1, player2, empty], dtype=np.uint8).reshape((1, 3,6,7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndOccwx_mnjq"
      },
      "outputs": [],
      "source": [
        "n_inputs = 42\n",
        "n_outputs = 7\n",
        "\n",
        "TrainNet = DQN(DeepNetworkModel(input_layer, hidden_layers, n_outputs), gamma, learning_rate, batch_size, max_memory, min_memory)\n",
        "TargetNet = DQN(DeepNetworkModel(input_layer, hidden_layers, n_outputs), gamma, learning_rate, batch_size, max_memory, min_memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1untBbGIktY8",
        "outputId": "6f75d0b5-386d-43ee-e098-65ed695aa2bb"
      },
      "outputs": [],
      "source": [
        "pbar = tqdm.tqdm(range(episodes))\n",
        "min_epsilon = 0.05\n",
        "p1, p2, illigal_p1, illigal_p2 = [0], [0], [0], [0]\n",
        "random_agent = AgentRandom()\n",
        "agents = [TrainNet, random_agent]\n",
        "best_winrate = 0\n",
        "for n in pbar:\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_multiplier) \n",
        "    turn, reward = play_epiode_one_sided(env, agents, TargetNet, epsilon)\n",
        "    if n % (episodes*0.005) == 0 and n!=0:\n",
        "          \n",
        "          dqn_agent = AgentDQN(TrainNet, False)\n",
        "          test_agents = [dqn_agent, random_agent]\n",
        "          p1_win, p2_win, illigal_p1_move, _ = test_winrate(env, test_agents, 100)\n",
        "          p1.append(p1_win)\n",
        "          p2.append(p2_win)\n",
        "    total_rewards[n] = reward\n",
        "    total_turns[n] = turn\n",
        "    pbar.set_postfix({\n",
        "        'epsilon': epsilon,\n",
        "        'turns' : turn,\n",
        "        'ai_winrate': p1[-1],\n",
        "        'random_winrate': p2[-1],\n",
        "        'draw' : 1-p1[-1]-p2[-1]- illigal_p1[-1],\n",
        "        'illegal_ai': illigal_p1[-1]\n",
        "    })\n",
        "    copy_iter += 1\n",
        "    if copy_iter % copy_step == 0:\n",
        "        TargetNet.copy_weights(TrainNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "5Wp8RjS7GhMy",
        "outputId": "92fe7f01-6ef4-4e8a-a28e-97eb7d08d77c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(p1)\n",
        "plt.plot(p2)\n",
        "plt.plot(1-np.array(p1)-np.array(p2))\n",
        "plt.plot(np.ones(len(p1))*0.8, 'r-')\n",
        "plt.plot(np.ones(len(p1))*0.9, 'r--')\n",
        "plt.plot(np.ones(len(p1)), 'r-.')\n",
        "plt.legend([\"player1_wins\", \"player2_wins\", \"draws\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phMbbhAcmnjr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "mean_loss = np.zeros(len(TrainNet.loss_list))\n",
        "for i in range(0, len(TrainNet.loss_list), 100):\n",
        "    mean_loss[i] = np.mean(TrainNet.loss_list[max(0, i-1000):(i+1)])\n",
        "\n",
        "total_turns_ = np.zeros(len(total_turns))\n",
        "for i in range(len(total_turns_)):\n",
        "    total_turns_[i] = np.mean(total_turns[max(0, i-1000):(i+1)])\n",
        "\n",
        "mloss = mean_loss\n",
        "mloss = np.where(mloss == 0, 0.00001, mloss)\n",
        "\n",
        "plt.plot(mloss[1:])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(total_turns_[200:])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "B-EQnEC_9eij",
        "outputId": "f8ed4983-7611-4a66-fdb6-95c6e0489618"
      },
      "outputs": [],
      "source": [
        "random_agent = AgentRandom()\n",
        "dqn_agent = AgentDQN(TrainNet)\n",
        "agents = [dqn_agent, random_agent]\n",
        "test_winrate(env, agents, 1000, swap_sides=False, info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM_l1FP2-lb9",
        "outputId": "7a6f0a30-3b92-40ba-93a0-b07b05573694"
      },
      "outputs": [],
      "source": [
        "random_agent = AgentRandom()\n",
        "dqn_agent = AgentDQN(TrainNet, illigal=False)\n",
        "agents = [random_agent, dqn_agent]\n",
        "test_winrate(env, agents, 1000, False, True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "153ef6217985742c060c1ee4e18d16bdfde0791d84e6e718627ed55c1d8e6ea6"
    },
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
